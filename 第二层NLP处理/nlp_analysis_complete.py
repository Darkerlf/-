"""
äºšé©¬é€Šå¨åˆ€è¯„è®ºNLPå®Œæ•´åˆ†æç³»ç»Ÿ
åŒ…å«: BERTæƒ…æ„Ÿåˆ†æã€ABSAã€TextRankã€NERã€ä¸»é¢˜å»ºæ¨¡ã€å¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•
2. å®‰è£…ä¾èµ–: pip install -r requirements.txt
3. è¿è¡Œ: python nlp_analysis_complete.py

ä½œè€…: [Your Name]
æ—¥æœŸ: 2026-01-29
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import json
from datetime import datetime
from collections import Counter
import re

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å’Œæ ·å¼
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")
sns.set_palette("husl")

# ==================== é…ç½® ====================
class Config:
    """é…ç½®å‚æ•°"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
    REVIEWS_FILE = 'reviews_cleaned.csv'
    FACT_REVIEWS_FILE = 'fact_review_enriched.csv'
    PRODUCTS_FILE = 'products_clean.csv'
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = 'nlp_results'
    VIZ_DIR = 'nlp_results/visualizations'
    DATA_DIR = 'nlp_results/data'
    
    # åˆ†æå‚æ•°
    MIN_REVIEW_LENGTH = 20  # æœ€å°è¯„è®ºé•¿åº¦
    N_TOPICS = 5  # ä¸»é¢˜æ•°é‡
    TOP_KEYWORDS = 20  # å…³é”®è¯æ•°é‡
    
    # å¨åˆ€æ–¹é¢å®šä¹‰
    ASPECTS = {
        'sharpness': ['sharp', 'blade', 'edge', 'dull', 'cutting', 'razor', 'keen'],
        'quality': ['quality', 'made', 'construction', 'build', 'material', 'craftsmanship'],
        'durability': ['durable', 'last', 'lasting', 'sturdy', 'strong', 'break', 'broke', 'chip'],
        'handle': ['handle', 'grip', 'comfortable', 'ergonomic', 'hold', 'hand'],
        'rust': ['rust', 'rusted', 'corrosion', 'stain', 'stainless', 'oxidation'],
        'balance': ['balance', 'balanced', 'weight', 'heavy', 'light'],
        'value': ['price', 'value', 'money', 'worth', 'expensive', 'cheap', 'cost'],
        'appearance': ['look', 'beautiful', 'pretty', 'appearance', 'design', 'aesthetic']
    }

# åˆ›å»ºè¾“å‡ºç›®å½•
def setup_directories():
    """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
    Path(Config.OUTPUT_DIR).mkdir(exist_ok=True)
    Path(Config.VIZ_DIR).mkdir(exist_ok=True)
    Path(Config.DATA_DIR).mkdir(exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {Config.OUTPUT_DIR}")

# ==================== æ•°æ®åŠ è½½ ====================
def load_data():
    """åŠ è½½æ•°æ®"""
    print("\n" + "="*80)
    print("ã€ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ã€‘")
    print("="*80)
    
    try:
        reviews = pd.read_csv(Config.REVIEWS_FILE, encoding='utf-8-sig')
        fact_reviews = pd.read_csv(Config.FACT_REVIEWS_FILE, encoding='utf-8-sig')
        products = pd.read_csv(Config.PRODUCTS_FILE, encoding='utf-8-sig')
        
        print(f"âœ“ è¯„è®ºæ•°æ®: {len(reviews):,} æ¡")
        print(f"âœ“ å¢å¼ºè¯„è®º: {len(fact_reviews):,} æ¡")
        print(f"âœ“ äº§å“æ•°æ®: {len(products):,} ä¸ª")
        
        # åªä¿ç•™æœ‰æ–‡æœ¬çš„è¯„è®º
        text_reviews = reviews[reviews['has_text'] == 1].copy()
        print(f"âœ“ æœ‰æ•ˆæ–‡æœ¬è¯„è®º: {len(text_reviews):,} æ¡")
        
        return text_reviews, fact_reviews, products
        
    except FileNotFoundError as e:
        print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ - {e}")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•:")
        print(f"  - {Config.REVIEWS_FILE}")
        print(f"  - {Config.FACT_REVIEWS_FILE}")
        print(f"  - {Config.PRODUCTS_FILE}")
        raise

# ==================== 1. BERTæƒ…æ„Ÿåˆ†æ ====================
def bert_sentiment_analysis(reviews_df):
    """ä½¿ç”¨BERTè¿›è¡Œæƒ…æ„Ÿåˆ†æ"""
    print("\n" + "="*80)
    print("ã€ç¬¬äºŒæ­¥ï¼šBERTæƒ…æ„Ÿåˆ†æã€‘")
    print("="*80)
    
    try:
        from transformers import pipeline
        import torch
        
        print("æ­£åœ¨åŠ è½½BERTæ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ï¼Œçº¦500MBï¼‰...")
        
        # ä½¿ç”¨CPUå‹å¥½çš„DistilBERT
        sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=-1  # ä½¿ç”¨CPU
        )
        
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ‰¹é‡å¤„ç†
        texts = reviews_df['review_text_clean'].fillna('').tolist()
        batch_size = 32
        results = []
        
        print(f"å¼€å§‹åˆ†æ {len(texts)} æ¡è¯„è®º...")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            # æˆªæ–­é•¿æ–‡æœ¬
            batch = [text[:512] for text in batch]
            batch_results = sentiment_analyzer(batch)
            results.extend(batch_results)
            
            if (i + batch_size) % 500 == 0:
                print(f"  è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")
        
        # æ•´ç†ç»“æœ
        reviews_df['bert_label'] = [r['label'] for r in results]
        reviews_df['bert_score'] = [r['score'] for r in results]
        
        # ç»Ÿè®¡
        label_dist = reviews_df['bert_label'].value_counts()
        print(f"\nâœ“ BERTæƒ…æ„Ÿåˆ†æå®Œæˆ!")
        print(f"\næƒ…æ„Ÿåˆ†å¸ƒ:")
        for label, count in label_dist.items():
            pct = count / len(reviews_df) * 100
            print(f"  {label}: {count:,} ({pct:.1f}%)")
        
        # ä¿å­˜ç»“æœ
        output_file = f"{Config.DATA_DIR}/bert_sentiment_results.csv"
        reviews_df[['review_id', 'review_text_clean', 'review_rating', 
                    'bert_label', 'bert_score']].to_csv(output_file, index=False)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_file}")
        
        return reviews_df
        
    except Exception as e:
        print(f"âœ— BERTåˆ†æå¤±è´¥: {e}")
        print("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æ")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šè§„åˆ™åˆ†æ
        positive_words = {'excellent', 'great', 'amazing', 'perfect', 'love', 'best'}
        negative_words = {'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate'}
        
        def simple_sentiment(text):
            text = str(text).lower()
            pos = sum(1 for w in positive_words if w in text)
            neg = sum(1 for w in negative_words if w in text)
            return 'POSITIVE' if pos > neg else ('NEGATIVE' if neg > pos else 'NEUTRAL')
        
        reviews_df['bert_label'] = reviews_df['review_text_clean'].apply(simple_sentiment)
        reviews_df['bert_score'] = 0.5
        
        return reviews_df

# ==================== 2. ABSAæ–¹é¢çº§æƒ…æ„Ÿåˆ†æ ====================
def absa_analysis(reviews_df):
    """æ–¹é¢çº§æƒ…æ„Ÿåˆ†æ"""
    print("\n" + "="*80)
    print("ã€ç¬¬ä¸‰æ­¥ï¼šABSAæ–¹é¢çº§æƒ…æ„Ÿåˆ†æã€‘")
    print("="*80)
    
    print(f"åˆ†æ {len(Config.ASPECTS)} ä¸ªäº§å“æ–¹é¢...")
    
    aspect_data = []
    
    for idx, row in reviews_df.iterrows():
        text = str(row['review_text_clean']).lower()
        rating = row['review_rating']
        bert_label = row.get('bert_label', 'NEUTRAL')
        
        for aspect_name, keywords in Config.ASPECTS.items():
            # æ£€æŸ¥æ˜¯å¦æåˆ°è¯¥æ–¹é¢
            mentioned = any(kw in text for kw in keywords)
            
            if mentioned:
                # æå–ç›¸å…³å¥å­
                sentences = text.split('.')
                relevant_sentences = [s for s in sentences if any(kw in s for kw in keywords)]
                
                if relevant_sentences:
                    # ç®€å•æƒ…æ„Ÿåˆ¤æ–­ï¼šåŸºäºæ˜Ÿçº§å’ŒBERTç»“æœ
                    if rating >= 4 and bert_label == 'POSITIVE':
                        sentiment = 'positive'
                        score = 1.0
                    elif rating <= 2 or bert_label == 'NEGATIVE':
                        sentiment = 'negative'
                        score = -1.0
                    else:
                        sentiment = 'neutral'
                        score = 0.0
                    
                    aspect_data.append({
                        'review_id': row['review_id'],
                        'aspect': aspect_name,
                        'sentiment': sentiment,
                        'score': score,
                        'rating': rating,
                        'sample_text': relevant_sentences[0][:100]
                    })
        
        if (idx + 1) % 500 == 0:
            print(f"  è¿›åº¦: {idx+1}/{len(reviews_df)}")
    
    aspect_df = pd.DataFrame(aspect_data)
    
    # ç»Ÿè®¡æ¯ä¸ªæ–¹é¢
    aspect_stats = aspect_df.groupby('aspect').agg({
        'score': ['mean', 'count'],
        'rating': 'mean'
    }).round(3)
    
    aspect_stats.columns = ['avg_sentiment', 'mention_count', 'avg_rating']
    aspect_stats['mention_rate'] = (aspect_stats['mention_count'] / len(reviews_df) * 100).round(1)
    aspect_stats = aspect_stats.sort_values('mention_count', ascending=False)
    
    print(f"\nâœ“ æ–¹é¢çº§åˆ†æå®Œæˆ!")
    print(f"\n{aspect_stats.to_string()}")
    
    # ä¿å­˜ç»“æœ
    aspect_df.to_csv(f"{Config.DATA_DIR}/absa_detailed.csv", index=False)
    aspect_stats.to_csv(f"{Config.DATA_DIR}/absa_summary.csv")
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {Config.DATA_DIR}/absa_*.csv")
    
    return aspect_df, aspect_stats

# ==================== 3. TextRankå…³é”®è¯æå– ====================
def textrank_keywords(reviews_df):
    """ä½¿ç”¨TextRankæå–å…³é”®è¯"""
    print("\n" + "="*80)
    print("ã€ç¬¬å››æ­¥ï¼šTextRankå…³é”®è¯æå–ã€‘")
    print("="*80)
    
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import networkx as nx
        
        # åˆå¹¶æ‰€æœ‰è¯„è®º
        all_text = ' '.join(reviews_df['review_text_clean'].fillna('').tolist())
        
        # åˆ†è¯
        words = re.findall(r'\b[a-z]{3,}\b', all_text.lower())
        
        # åœç”¨è¯
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been'
        }
        
        words = [w for w in words if w not in stop_words and len(w) > 3]
        
        print(f"å¤„ç† {len(words):,} ä¸ªè¯...")
        
        # æ„å»ºå…±ç°å›¾
        graph = nx.Graph()
        window_size = 5
        
        for i in range(len(words) - window_size):
            for j in range(i + 1, i + window_size):
                if words[i] != words[j]:
                    if graph.has_edge(words[i], words[j]):
                        graph[words[i]][words[j]]['weight'] += 1
                    else:
                        graph.add_edge(words[i], words[j], weight=1)
        
        print(f"å›¾èŠ‚ç‚¹æ•°: {len(graph.nodes())}")
        
        # PageRank
        scores = nx.pagerank(graph, weight='weight')
        
        # æ’åº
        keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:Config.TOP_KEYWORDS]
        
        print(f"\nâœ“ TextRankå…³é”®è¯æå–å®Œæˆ!")
        print(f"\nTop {Config.TOP_KEYWORDS} å…³é”®è¯:")
        for i, (word, score) in enumerate(keywords, 1):
            print(f"  {i:2}. {word:15} - {score:.6f}")
        
        # ä¿å­˜ç»“æœ
        keywords_df = pd.DataFrame(keywords, columns=['keyword', 'score'])
        keywords_df.to_csv(f"{Config.DATA_DIR}/textrank_keywords.csv", index=False)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {Config.DATA_DIR}/textrank_keywords.csv")
        
        return keywords_df
        
    except Exception as e:
        print(f"âœ— TextRankå¤±è´¥: {e}")
        print("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•è¯é¢‘ç»Ÿè®¡")
        
        from collections import Counter
        all_text = ' '.join(reviews_df['review_text_clean'].fillna('').tolist())
        words = re.findall(r'\b[a-z]{4,}\b', all_text.lower())
        word_freq = Counter(words).most_common(Config.TOP_KEYWORDS)
        
        keywords_df = pd.DataFrame(word_freq, columns=['keyword', 'frequency'])
        return keywords_df

# ==================== 4. ç®€å•NER ====================
def simple_ner(reviews_df):
    """ç®€å•çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆå“ç‰Œå’Œæè´¨ï¼‰"""
    print("\n" + "="*80)
    print("ã€ç¬¬äº”æ­¥ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‘")
    print("="*80)
    
    # å¸¸è§å“ç‰Œå’Œæè´¨
    brands = [
        'wusthof', 'shun', 'victorinox', 'zwilling', 'henckels', 'global',
        'miyabi', 'dalstrong', 'cuisinart', 'farberware', 'imarku', 'paudin',
        'hoshanho', 'mercer', 'chicago cutlery', 'j.a. henckels', 'cutco'
    ]
    
    materials = [
        'steel', 'stainless', 'carbon', 'damascus', 'ceramic', 'titanium',
        'german steel', 'japanese steel', 'high carbon', 'vg-10', 'vg-max',
        'aus-8', 'x50crmov15'
    ]
    
    print("è¯†åˆ«å“ç‰Œå’Œæè´¨...")
    
    all_brands = []
    all_materials = []
    
    for text in reviews_df['review_text_clean'].fillna(''):
        text_lower = text.lower()
        all_brands.extend([b for b in brands if b in text_lower])
        all_materials.extend([m for m in materials if m in text_lower])
    
    brand_counts = Counter(all_brands)
    material_counts = Counter(all_materials)
    
    print(f"\nâœ“ NERå®Œæˆ!")
    print(f"\nå“ç‰ŒæåŠ Top 10:")
    for i, (brand, count) in enumerate(brand_counts.most_common(10), 1):
        print(f"  {i:2}. {brand:20} - {count:3} æ¬¡")
    
    print(f"\næè´¨æåŠ Top 10:")
    for i, (material, count) in enumerate(material_counts.most_common(10), 1):
        print(f"  {i:2}. {material:20} - {count:3} æ¬¡")
    
    # ä¿å­˜ç»“æœ
    brand_df = pd.DataFrame(brand_counts.most_common(), columns=['brand', 'count'])
    material_df = pd.DataFrame(material_counts.most_common(), columns=['material', 'count'])
    
    brand_df.to_csv(f"{Config.DATA_DIR}/ner_brands.csv", index=False)
    material_df.to_csv(f"{Config.DATA_DIR}/ner_materials.csv", index=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {Config.DATA_DIR}/ner_*.csv")
    
    return brand_df, material_df

# ==================== 5. ä¸»é¢˜å»ºæ¨¡ ====================
def topic_modeling(reviews_df):
    """LDAå’ŒNMFä¸»é¢˜å»ºæ¨¡"""
    print("\n" + "="*80)
    print("ã€ç¬¬å…­æ­¥ï¼šä¸»é¢˜å»ºæ¨¡ï¼ˆLDA + NMFï¼‰ã€‘")
    print("="*80)
    
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    from sklearn.decomposition import LatentDirichletAllocation, NMF
    
    # åªä½¿ç”¨é•¿è¯„è®º
    long_reviews = reviews_df[reviews_df['text_len'] > Config.MIN_REVIEW_LENGTH]
    texts = long_reviews['review_text_clean'].fillna('').tolist()
    
    print(f"ä½¿ç”¨ {len(texts):,} æ¡é•¿è¯„è®ºè¿›è¡Œä¸»é¢˜å»ºæ¨¡...")
    
    # ===== LDA =====
    print("\næ‰§è¡ŒLDAä¸»é¢˜å»ºæ¨¡...")
    vectorizer_lda = CountVectorizer(
        max_features=500,
        stop_words='english',
        min_df=3,
        max_df=0.7
    )
    
    doc_term_matrix = vectorizer_lda.fit_transform(texts)
    
    lda = LatentDirichletAllocation(
        n_components=Config.N_TOPICS,
        random_state=42,
        max_iter=20
    )
    
    lda.fit(doc_term_matrix)
    
    # æå–LDAä¸»é¢˜
    feature_names = vectorizer_lda.get_feature_names_out()
    lda_topics = []
    
    print(f"\nLDAå‘ç°çš„ {Config.N_TOPICS} ä¸ªä¸»é¢˜:")
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[-10:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        lda_topics.append({
            'topic_id': topic_idx,
            'method': 'LDA',
            'top_words': ', '.join(top_words[:8])
        })
        print(f"  ä¸»é¢˜ {topic_idx + 1}: {', '.join(top_words[:8])}")
    
    # ===== NMF =====
    print("\næ‰§è¡ŒNMFä¸»é¢˜å»ºæ¨¡...")
    vectorizer_nmf = TfidfVectorizer(
        max_features=500,
        stop_words='english',
        min_df=3,
        max_df=0.7
    )
    
    tfidf_matrix = vectorizer_nmf.fit_transform(texts)
    
    nmf = NMF(
        n_components=Config.N_TOPICS,
        random_state=42,
        max_iter=200
    )
    
    nmf.fit(tfidf_matrix)
    
    # æå–NMFä¸»é¢˜
    feature_names_nmf = vectorizer_nmf.get_feature_names_out()
    nmf_topics = []
    
    print(f"\nNMFå‘ç°çš„ {Config.N_TOPICS} ä¸ªä¸»é¢˜:")
    for topic_idx, topic in enumerate(nmf.components_):
        top_words_idx = topic.argsort()[-10:][::-1]
        top_words = [feature_names_nmf[i] for i in top_words_idx]
        nmf_topics.append({
            'topic_id': topic_idx,
            'method': 'NMF',
            'top_words': ', '.join(top_words[:8])
        })
        print(f"  ä¸»é¢˜ {topic_idx + 1}: {', '.join(top_words[:8])}")
    
    # ä¿å­˜ç»“æœ
    topics_df = pd.DataFrame(lda_topics + nmf_topics)
    topics_df.to_csv(f"{Config.DATA_DIR}/topic_modeling.csv", index=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {Config.DATA_DIR}/topic_modeling.csv")
    
    return lda_topics, nmf_topics

# ==================== å¯è§†åŒ– ====================
def create_visualizations(reviews_df, aspect_stats, keywords_df, brand_df, material_df):
    """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "="*80)
    print("ã€ç¬¬ä¸ƒæ­¥ï¼šç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‘")
    print("="*80)
    
    # 1. BERTæƒ…æ„Ÿåˆ†å¸ƒ
    print("1. BERTæƒ…æ„Ÿåˆ†å¸ƒå›¾...")
    plt.figure(figsize=(10, 6))
    sentiment_counts = reviews_df['bert_label'].value_counts()
    colors = ['#2ecc71', '#e74c3c', '#95a5a6']
    plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',
            colors=colors, startangle=90)
    plt.title('BERT Sentiment Distribution', fontsize=16, fontweight='bold')
    plt.savefig(f"{Config.VIZ_DIR}/1_bert_sentiment_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. BERTæƒ…æ„Ÿvsæ˜Ÿçº§
    print("2. æƒ…æ„Ÿvsæ˜Ÿçº§å¯¹æ¯”å›¾...")
    plt.figure(figsize=(12, 6))
    sentiment_by_rating = reviews_df.groupby(['review_rating', 'bert_label']).size().unstack(fill_value=0)
    sentiment_by_rating.plot(kind='bar', stacked=False, color=['#2ecc71', '#e74c3c'])
    plt.title('Sentiment Distribution by Star Rating', fontsize=16, fontweight='bold')
    plt.xlabel('Star Rating', fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.legend(title='BERT Sentiment')
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/2_sentiment_by_rating.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ABSAæ–¹é¢æƒ…æ„Ÿçƒ­åŠ›å›¾
    print("3. ABSAæ–¹é¢æƒ…æ„Ÿçƒ­åŠ›å›¾...")
    plt.figure(figsize=(10, 8))
    heatmap_data = aspect_stats[['avg_sentiment', 'mention_rate']].sort_values('mention_rate', ascending=False)
    sns.heatmap(heatmap_data.T, annot=True, fmt='.2f', cmap='RdYlGn', center=0,
                cbar_kws={'label': 'Score'})
    plt.title('ABSA: Aspect Sentiment Heatmap', fontsize=16, fontweight='bold')
    plt.ylabel('Metric', fontsize=12)
    plt.xlabel('Aspect', fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/3_absa_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. ABSAæ–¹é¢æåŠç‡
    print("4. ABSAæ–¹é¢æåŠç‡...")
    plt.figure(figsize=(12, 6))
    aspect_stats_sorted = aspect_stats.sort_values('mention_rate', ascending=True)
    colors_aspect = ['#2ecc71' if x > 0 else '#e74c3c' for x in aspect_stats_sorted['avg_sentiment']]
    plt.barh(aspect_stats_sorted.index, aspect_stats_sorted['mention_rate'], color=colors_aspect)
    plt.xlabel('Mention Rate (%)', fontsize=12)
    plt.title('ABSA: Aspect Mention Rates', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/4_absa_mention_rates.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. TextRankå…³é”®è¯è¯äº‘
    print("5. TextRankå…³é”®è¯å¯è§†åŒ–...")
    plt.figure(figsize=(14, 8))
    if len(keywords_df) > 0:
        keywords_top20 = keywords_df.head(20)
        plt.barh(range(len(keywords_top20)), keywords_top20.iloc[:, 1].values)
        plt.yticks(range(len(keywords_top20)), keywords_top20.iloc[:, 0].values)
        plt.xlabel('Score/Frequency', fontsize=12)
        plt.title('Top 20 Keywords (TextRank)', fontsize=16, fontweight='bold')
        plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/5_textrank_keywords.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 6. å“ç‰ŒæåŠ
    print("6. å“ç‰ŒæåŠç»Ÿè®¡...")
    plt.figure(figsize=(12, 6))
    if len(brand_df) > 0:
        top_brands = brand_df.head(10)
        plt.bar(top_brands['brand'], top_brands['count'], color='steelblue')
        plt.xlabel('Brand', fontsize=12)
        plt.ylabel('Mention Count', fontsize=12)
        plt.title('Top 10 Brand Mentions in Reviews', fontsize=16, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/6_brand_mentions.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 7. æè´¨æåŠ
    print("7. æè´¨æåŠç»Ÿè®¡...")
    plt.figure(figsize=(12, 6))
    if len(material_df) > 0:
        top_materials = material_df.head(10)
        plt.bar(top_materials['material'], top_materials['count'], color='coral')
        plt.xlabel('Material', fontsize=12)
        plt.ylabel('Mention Count', fontsize=12)
        plt.title('Top 10 Material Mentions in Reviews', fontsize=16, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/7_material_mentions.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 8. è¯„è®ºé•¿åº¦åˆ†å¸ƒ
    print("8. è¯„è®ºé•¿åº¦åˆ†å¸ƒ...")
    plt.figure(figsize=(12, 6))
    plt.hist(reviews_df['text_len'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)
    plt.xlabel('Review Length (characters)', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.title('Review Length Distribution', fontsize=16, fontweight='bold')
    plt.axvline(reviews_df['text_len'].median(), color='red', linestyle='--', 
                label=f'Median: {reviews_df["text_len"].median():.0f}')
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{Config.VIZ_DIR}/8_review_length_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\nâœ“ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {Config.VIZ_DIR}/")
    print(f"  å…±ç”Ÿæˆ 8 å¼ å¯è§†åŒ–å›¾è¡¨")

# ==================== ç”ŸæˆæŠ¥å‘Š ====================
def generate_summary_report(reviews_df, aspect_stats, keywords_df, brand_df, material_df):
    """ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ã€ç¬¬å…«æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Šã€‘")
    print("="*80)
    
    report = {
        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_summary': {
            'total_reviews': int(len(reviews_df)),
            'avg_review_length': float(reviews_df['text_len'].mean()),
            'date_range': {
                'start': str(reviews_df['review_date_dt'].min()),
                'end': str(reviews_df['review_date_dt'].max())
            }
        },
        'bert_sentiment': {
            'positive': int((reviews_df['bert_label'] == 'POSITIVE').sum()),
            'negative': int((reviews_df['bert_label'] == 'NEGATIVE').sum()),
            'positive_rate': float((reviews_df['bert_label'] == 'POSITIVE').sum() / len(reviews_df) * 100)
        },
        'absa_insights': {
            'most_mentioned_aspect': aspect_stats.index[0],
            'most_positive_aspect': aspect_stats['avg_sentiment'].idxmax(),
            'most_negative_aspect': aspect_stats['avg_sentiment'].idxmin(),
            'aspect_stats': aspect_stats.to_dict()
        },
        'top_keywords': keywords_df.head(10).to_dict('records'),
        'top_brands': brand_df.head(5).to_dict('records'),
        'top_materials': material_df.head(5).to_dict('records')
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    with open(f"{Config.OUTPUT_DIR}/analysis_summary.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = f"""# Amazon Kitchen Knife Reviews - NLP Analysis Report

**Generated**: {report['analysis_date']}

## Data Summary
- Total Reviews: {report['data_summary']['total_reviews']:,}
- Average Review Length: {report['data_summary']['avg_review_length']:.1f} characters
- Date Range: {report['data_summary']['date_range']['start']} to {report['data_summary']['date_range']['end']}

## BERT Sentiment Analysis
- Positive Reviews: {report['bert_sentiment']['positive']:,} ({report['bert_sentiment']['positive_rate']:.1f}%)
- Negative Reviews: {report['bert_sentiment']['negative']:,}

## ABSA Insights
- Most Mentioned Aspect: **{report['absa_insights']['most_mentioned_aspect']}**
- Most Positive Aspect: **{report['absa_insights']['most_positive_aspect']}**
- Most Negative Aspect: **{report['absa_insights']['most_negative_aspect']}**

## Top Keywords
{chr(10).join([f"{i+1}. {kw['keyword']}" for i, kw in enumerate(report['top_keywords'])])}

## Top Brand Mentions
{chr(10).join([f"{i+1}. {b['brand']}: {b['count']} mentions" for i, b in enumerate(report['top_brands'])])}

## Top Material Mentions
{chr(10).join([f"{i+1}. {m['material']}: {m['count']} mentions" for i, m in enumerate(report['top_materials'])])}

---
*Report generated by Amazon Review NLP Analysis System*
"""
    
    with open(f"{Config.OUTPUT_DIR}/ANALYSIS_REPORT.md", 'w', encoding='utf-8') as f:
        f.write(md_report)
    
    print(f"âœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {Config.OUTPUT_DIR}/analysis_summary.json")
    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {Config.OUTPUT_DIR}/ANALYSIS_REPORT.md")
    
    return report

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ” äºšé©¬é€Šå¨åˆ€è¯„è®ºNLPå®Œæ•´åˆ†æç³»ç»Ÿ")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 0. åˆ›å»ºç›®å½•
    setup_directories()
    
    # 1. åŠ è½½æ•°æ®
    reviews_df, fact_reviews, products = load_data()
    
    # 2. BERTæƒ…æ„Ÿåˆ†æ
    reviews_df = bert_sentiment_analysis(reviews_df)
    
    # 3. ABSAæ–¹é¢çº§æƒ…æ„Ÿ
    aspect_df, aspect_stats = absa_analysis(reviews_df)
    
    # 4. TextRankå…³é”®è¯
    keywords_df = textrank_keywords(reviews_df)
    
    # 5. ç®€å•NER
    brand_df, material_df = simple_ner(reviews_df)
    
    # 6. ä¸»é¢˜å»ºæ¨¡
    lda_topics, nmf_topics = topic_modeling(reviews_df)
    
    # 7. ç”Ÿæˆå¯è§†åŒ–
    create_visualizations(reviews_df, aspect_stats, keywords_df, brand_df, material_df)
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    report = generate_summary_report(reviews_df, aspect_stats, keywords_df, brand_df, material_df)
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰åˆ†æå®Œæˆ!")
    print("="*80)
    print(f"\nç»“æœä¿å­˜ä½ç½®:")
    print(f"  ğŸ“ ä¸»ç›®å½•: {Config.OUTPUT_DIR}/")
    print(f"  ğŸ“Š æ•°æ®æ–‡ä»¶: {Config.DATA_DIR}/")
    print(f"  ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {Config.VIZ_DIR}/")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - analysis_summary.json (å®Œæ•´åˆ†æç»“æœ)")
    print(f"  - ANALYSIS_REPORT.md (åˆ†ææŠ¥å‘Š)")
    print(f"  - 8å¼ å¯è§†åŒ–å›¾è¡¨")
    print(f"  - 7ä¸ªCSVæ•°æ®æ–‡ä»¶")
    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
